{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Downloads/Datasets/candidates_tweets.csv') #raw tweet data\n",
    "\n",
    "df.rename(columns={'text': 'original_tweets'}, inplace=True) #retain originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_tweet_text(text):\n",
    "    \"\"\"cleans data for clarity and uniformity\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = contractions.fix(text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('â€”', ' ') # two types of hyphens\n",
    "    text = text.replace('-', ' ')\n",
    "    text = re.sub('@[A-Za-z0-9_]\\S+', '', text)\n",
    "    text = re.sub('#[A-Za-z0-9_]\\S+', '', text)\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    text = re.sub('[^a-zA-Z0-9 -]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['processed_tweets'] = df['original_tweets'] # copy tweet text over to a new column and then clean the text of the entire column\n",
    "df['processed_tweets'] = df['processed_tweets'].apply(lambda x: process_tweet_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('processed_canditate_tweets.csv') #save results for the record before I mess with the text contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break up primary dataframe into individual datframes\n",
    "JoeBiden = df.loc[df['name'] == 'JoeBiden']\n",
    "ElizabethWarren = df.loc[df['name'] == 'ewarren']\n",
    "BernieSanders = df.loc[df['name'] == 'BernieSanders']\n",
    "MikeBloomberg = df.loc[df['name'] == 'MikeBloomberg']\n",
    "PeteButtigieg = df.loc[df['name'] == 'PeteButtigieg']\n",
    "AmyKlobuchar = df.loc[df['name'] == 'amyklobuchar']\n",
    "DonaldTrump = df.loc[df['name'] == 'realDonaldTrump']\n",
    "Democrats = df.loc[df['name'] != 'realDonaldTrump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize_ngrams(candidate):\n",
    "    \n",
    "    \"\"\"This tokenizes the text as well as calculates ngrams to find common phrases\"\"\"\n",
    "    \n",
    "    # put all of the tweets together in an object called 'text_all'\n",
    "    text_all = candidate['processed_tweets'].str.cat(sep=' ')\n",
    "\n",
    "    # intialize tokenizer and tokenize all of the text\n",
    "    tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text_all.lower())\n",
    "\n",
    "    # eliminate stop words from text\n",
    "    cached_stop_words = stopwords.words(\"english\")\n",
    "    filtered_tokens = [w for w in tokens if not w in cached_stop_words] \n",
    "    \n",
    "    # find most common phrases of 2 & 3 words and store them together in ngrams\n",
    "    bi_gram = Counter(list(ngrams(filtered_tokens, 2)))\n",
    "    tri_gram = Counter(list(ngrams(filtered_tokens, 3)))\n",
    "    ngram = bi_gram.most_common(25) + tri_gram.most_common(25)\n",
    "\n",
    "    # reate dataframe to store data\n",
    "    results = pd.DataFrame(ngram, columns=['phrase', 'count'])\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the tweets of each candidate and create a \n",
    "# combined dateframe representing the most common phrases\n",
    "# for each political candidate.\n",
    "\n",
    "handles = [JoeBiden, ElizabethWarren, BernieSanders, \n",
    "           MikeBloomberg, PeteButtigieg, AmyKlobuchar,\n",
    "           Democrats, DonaldTrump]\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "for handle in handles:\n",
    "    df_new = tokenize_ngrams(handle)\n",
    "    results_df = pd.concat((results_df, df_new))\n",
    "    \n",
    "    \n",
    "# reformat tuple into a clean string for formatting purposes\n",
    "def convert_tuple(tup): \n",
    "    str =  ' '.join(tup) \n",
    "    str = '\"' + str + '\"'\n",
    "    return str\n",
    "\n",
    "\n",
    "results_df['phrase'] = results_df['phrase'].apply(lambda x: convert_tuple(x)) #apply to each row    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('most_common_phrases.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
